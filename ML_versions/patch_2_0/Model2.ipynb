{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "g4NTcBl2lV9r",
        "outputId": "1441233d-12e1-4dda-a520-3ac837215c68"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: sentence-transformers in /usr/local/lib/python3.12/dist-packages (5.1.2)\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.12/dist-packages (4.57.2)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.12/dist-packages (2.9.0+cu126)\n",
            "Requirement already satisfied: scikit-learn in /usr/local/lib/python3.12/dist-packages (1.6.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.12/dist-packages (2.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.67.1)\n",
            "Requirement already satisfied: scipy in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (1.16.3)\n",
            "Requirement already satisfied: huggingface-hub>=0.20.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (0.36.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (11.3.0)\n",
            "Requirement already satisfied: typing_extensions>=4.5.0 in /usr/local/lib/python3.12/dist-packages (from sentence-transformers) (4.15.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.12/dist-packages (from transformers) (3.20.0)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (25.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.12/dist-packages (from transformers) (6.0.3)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.12/dist-packages (from transformers) (2025.11.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.12/dist-packages (from transformers) (2.32.4)\n",
            "Requirement already satisfied: tokenizers<=0.23.0,>=0.22.0 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.22.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.12/dist-packages (from transformers) (0.7.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.12/dist-packages (from torch) (75.2.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.12/dist-packages (from torch) (1.14.0)\n",
            "Requirement already satisfied: networkx>=2.5.1 in /usr/local/lib/python3.12/dist-packages (from torch) (3.6)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.12/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec>=0.8.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.10.2.21 in /usr/local/lib/python3.12/dist-packages (from torch) (9.10.2.21)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.12/dist-packages (from torch) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.12/dist-packages (from torch) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.12/dist-packages (from torch) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.12/dist-packages (from torch) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.7.1 in /usr/local/lib/python3.12/dist-packages (from torch) (0.7.1)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.27.5 in /usr/local/lib/python3.12/dist-packages (from torch) (2.27.5)\n",
            "Requirement already satisfied: nvidia-nvshmem-cu12==3.3.20 in /usr/local/lib/python3.12/dist-packages (from torch) (3.3.20)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.12/dist-packages (from torch) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.12/dist-packages (from torch) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.5.0 in /usr/local/lib/python3.12/dist-packages (from torch) (3.5.0)\n",
            "Requirement already satisfied: joblib>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (1.5.2)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.12/dist-packages (from scikit-learn) (3.6.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /usr/local/lib/python3.12/dist-packages (from huggingface-hub>=0.20.0->sentence-transformers) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.12/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.12/dist-packages (from jinja2->torch) (3.0.3)\n",
            "Requirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.4.4)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (3.11)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2.5.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.12/dist-packages (from requests->transformers) (2025.11.12)\n"
          ]
        }
      ],
      "source": [
        "!pip install sentence-transformers transformers torch scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import json\n",
        "from google.colab import files\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from functools import lru_cache\n",
        "import warnings\n",
        "warnings.filterwarnings('ignore')\n",
        "\n",
        "class CSVStudentTopicMatcher:\n",
        "    \"\"\"Класс для сопоставления студентов и тем проектов на основе семантического анализа\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "        self._student_embeddings_cache = {}\n",
        "\n",
        "        # Словарь для нормализации специализаций с приоритетом по точному совпадению\n",
        "        self.specialization_mapping = {\n",
        "            'Machine Learning': ['Machine Learning', 'ML', 'AI', 'машинное обучение', 'ml', 'ai', 'machine learning'],\n",
        "            'Data Science': ['Data Science', 'Data Analytics', 'анализ данных', 'data science', 'data analytics'],\n",
        "            'NLP': ['NLP', 'Natural Language Processing', 'обработка текста', 'nlp', 'natural language processing'],\n",
        "            'Computer Vision': ['Computer Vision', 'CV', 'компьютерное зрение', 'computer vision', 'cv'],\n",
        "            'Data Engineering': ['Data Engineering', 'ETL', 'Big Data', 'инженерия данных', 'data engineering'],\n",
        "            'Backend': ['Backend', 'API', 'Microservices', 'Server-side', 'бэкенд', 'backend', 'back-end'],\n",
        "            'Frontend': ['Frontend', 'UI', 'UX', 'Web', 'React', 'Vue', 'фронтенд', 'frontend', 'front-end'],\n",
        "            'Android': ['Android', 'Mobile', 'Kotlin', 'мобильная разработка', 'android', 'mobile development'],\n",
        "            'DevOps': ['DevOps', 'Cloud', 'CI/CD', 'Infrastructure', 'девопс', 'devops'],\n",
        "            'QA': ['QA', 'Testing', 'Test Automation', 'Quality Assurance', 'тестирование', 'qa', 'quality assurance'],\n",
        "            'UI/UX': ['UI/UX', 'Design', 'User Experience', 'Interface', 'дизайн', 'ui/ux', 'ui', 'ux'],\n",
        "            'GameDev': ['GameDev', 'Game Development', 'VR', 'AR', 'геймдев', 'game development'],\n",
        "            'Биоинформатика': ['Биоинформатика', 'Bioinformatics', 'Genomics', 'Biology', 'геномика'],\n",
        "            'Cybersecurity': ['Cybersecurity', 'Security', 'InfoSec', 'кибербезопасность', 'cybersecurity'],\n",
        "            'Robotics': ['Robotics', 'Robots', 'Automation', 'робототехника'],\n",
        "            'Product Analytics': ['Product Analytics', 'Analytics', 'BI', 'Business Intelligence', 'аналитика'],\n",
        "            'Other': ['Other', 'Другое', 'Прочее', 'Разное']\n",
        "        }\n",
        "\n",
        "        # Создаем обратный индекс для быстрого поиска специализаций\n",
        "        self._spec_reverse_index = {}\n",
        "        for main_spec, variants in self.specialization_mapping.items():\n",
        "            for variant in variants:\n",
        "                self._spec_reverse_index[variant.lower()] = main_spec\n",
        "\n",
        "    @lru_cache(maxsize=1000)\n",
        "    def _normalize_specialization(self, specialization: str) -> str:\n",
        "        if pd.isna(specialization):\n",
        "            return 'Other'\n",
        "\n",
        "        spec_str = str(specialization).strip()\n",
        "        if not spec_str or spec_str.lower() in ['nan', 'none', '']:\n",
        "            return 'Other'\n",
        "\n",
        "        spec_lower = spec_str.lower()\n",
        "\n",
        "        # Проверяем точное совпадение в обратном индексе\n",
        "        if spec_lower in self._spec_reverse_index:\n",
        "            return self._spec_reverse_index[spec_lower]\n",
        "\n",
        "        # Проверяем частичное совпадение\n",
        "        for variant, main_spec in self._spec_reverse_index.items():\n",
        "            if variant in spec_lower or spec_lower in variant:\n",
        "                return main_spec\n",
        "\n",
        "        # Маппинг русских названий на английские\n",
        "        ru_en_map = {\n",
        "            'машинное обучение': 'Machine Learning',\n",
        "            'анализ данных': 'Data Science',\n",
        "            'обработка текста': 'NLP',\n",
        "            'компьютерное зрение': 'Computer Vision',\n",
        "            'инженерия данных': 'Data Engineering',\n",
        "            'бэкенд': 'Backend',\n",
        "            'фронтенд': 'Frontend',\n",
        "            'мобильная разработка': 'Android',\n",
        "            'девопс': 'DevOps',\n",
        "            'тестирование': 'QA',\n",
        "            'дизайн': 'UI/UX',\n",
        "            'геймдев': 'GameDev',\n",
        "            'биоинформатика': 'Биоинформатика',\n",
        "            'кибербезопасность': 'Cybersecurity',\n",
        "            'робототехника': 'Robotics',\n",
        "            'аналитика': 'Product Analytics'\n",
        "        }\n",
        "\n",
        "        for ru, en in ru_en_map.items():\n",
        "            if ru in spec_lower:\n",
        "                return en\n",
        "\n",
        "        return 'Other'\n",
        "\n",
        "    def _extract_skills(self, experience: str) -> List[str]:\n",
        "        if pd.isna(experience):\n",
        "            return []\n",
        "\n",
        "        experience_str = str(experience)\n",
        "        skills = set()  # Используем set для избежания дубликатов\n",
        "\n",
        "        # Оптимизированный словарь навыков с ключевыми словами\n",
        "        skill_keywords = {\n",
        "            'python': ['python', 'pytorch', 'tensorflow', 'keras', 'pandas', 'numpy', 'scikit-learn', 'sklearn'],\n",
        "            'java': ['java', 'spring', 'hibernate'],\n",
        "            'kotlin': ['kotlin'],\n",
        "            'sql': ['sql', 'postgresql', 'mysql', 'postgres', 'database'],\n",
        "            'javascript': ['javascript', 'js', 'react', 'vue', 'angular', 'typescript', 'node.js', 'nodejs', 'node'],\n",
        "            'docker': ['docker', 'container'],\n",
        "            'kubernetes': ['kubernetes', 'k8s'],\n",
        "            'ml': ['machine learning', 'ml', 'ai', 'нейросети', 'машинное обучение', 'deep learning'],\n",
        "            'nlp': ['nlp', 'natural language', 'текст', 'linguistics'],\n",
        "            'cv': ['computer vision', 'cv', 'image', 'vision', 'opencv', 'компьютерное зрение'],\n",
        "            'data': ['data science', 'data analysis', 'data engineering', 'анализ данных', 'big data'],\n",
        "            'web': ['web', 'frontend', 'backend', 'api', 'веб', 'website', 'fullstack'],\n",
        "            'mobile': ['mobile', 'android', 'ios', 'мобильный', 'react native', 'flutter'],\n",
        "            'devops': ['devops', 'ci/cd', 'cloud', 'aws', 'azure', 'gcp', 'github actions'],\n",
        "            'qa': ['qa', 'testing', 'test', 'quality', 'тестирование', 'selenium', 'automation']\n",
        "        }\n",
        "\n",
        "        experience_lower = experience_str.lower()\n",
        "\n",
        "        # Быстрый поиск навыков по ключевым словам\n",
        "        for skill, keywords in skill_keywords.items():\n",
        "            if any(keyword in experience_lower for keyword in keywords):\n",
        "                skills.add(skill)\n",
        "\n",
        "        return list(skills)\n",
        "\n",
        "    def _normalize_hours(self, hours) -> float:\n",
        "        try:\n",
        "            # Преобразуем в число, извлекая цифры из строки\n",
        "            if isinstance(hours, str):\n",
        "                numbers = re.findall(r'\\d+', hours)\n",
        "                hours = int(numbers[0]) if numbers else 0\n",
        "            else:\n",
        "                hours = int(float(hours))\n",
        "\n",
        "            # Бинаризация времени\n",
        "            if hours <= 10:\n",
        "                return 0.3\n",
        "            elif hours <= 15:\n",
        "                return 0.6\n",
        "            elif hours <= 20:\n",
        "                return 0.8\n",
        "            else:\n",
        "                return 1.0\n",
        "        except (ValueError, TypeError):\n",
        "            return 0.5  # Значение по умолчанию при ошибке\n",
        "\n",
        "    def _create_topic_text(self, topic_data: pd.Series) -> str:\n",
        "        text_parts = []\n",
        "        relevant_columns = ['title', 'description', 'required_specializations', 'tasks', 'goals']\n",
        "\n",
        "        for col in relevant_columns:\n",
        "            if col in topic_data and pd.notna(topic_data[col]):\n",
        "                text_parts.append(str(topic_data[col]))\n",
        "\n",
        "        return \" \".join(text_parts)\n",
        "\n",
        "    def _extract_keywords(self, topic_data: pd.Series) -> List[str]:\n",
        "        text = self._create_topic_text(topic_data)\n",
        "        # Извлекаем слова длиной от 3 символов, включая русские и английские\n",
        "        words = re.findall(r'\\b[a-zA-Zа-яА-ЯёЁ]{3,}\\b', text.lower())\n",
        "        return list(set(words))  # Убираем дубликаты\n",
        "\n",
        "    def calculate_semantic_similarity(self, student_texts: List[str], topic_text: str) -> np.ndarray:\n",
        "        # Используем кэш для избежания повторных вычислений эмбеддингов\n",
        "        cache_key = hash(tuple(student_texts))\n",
        "\n",
        "        if cache_key not in self._student_embeddings_cache:\n",
        "            # Пакетное вычисление эмбеддингов для всех студентов\n",
        "            student_embeddings = self.model.encode(student_texts, show_progress_bar=False)\n",
        "            self._student_embeddings_cache[cache_key] = student_embeddings\n",
        "        else:\n",
        "            student_embeddings = self._student_embeddings_cache[cache_key]\n",
        "\n",
        "        # Вычисляем эмбеддинг для темы (один раз)\n",
        "        topic_embedding = self.model.encode([topic_text])\n",
        "\n",
        "        # Вычисляем косинусное сходство\n",
        "        similarities = cosine_similarity(student_embeddings, topic_embedding)\n",
        "        return similarities.flatten()\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def calculate_specialization_match(self, student_spec: str, required_spec: str) -> float:\n",
        "        if student_spec == required_spec:\n",
        "            return 1.0\n",
        "\n",
        "        # Граф связанных специализаций\n",
        "        related_specs = {\n",
        "            'Machine Learning': ['Data Science', 'NLP', 'Computer Vision', 'Data Analytics'],\n",
        "            'Data Science': ['Machine Learning', 'Data Engineering', 'NLP', 'Product Analytics'],\n",
        "            'NLP': ['Machine Learning', 'Data Science'],\n",
        "            'Computer Vision': ['Machine Learning', 'Data Science'],\n",
        "            'Backend': ['DevOps', 'Data Engineering'],\n",
        "            'Frontend': ['UI/UX', 'Android'],\n",
        "            'Android': ['Frontend', 'UI/UX'],\n",
        "            'DevOps': ['Backend', 'Data Engineering'],\n",
        "            'Data Engineering': ['Data Science', 'Backend', 'DevOps'],\n",
        "            'UI/UX': ['Frontend', 'Android']\n",
        "        }\n",
        "\n",
        "        # Проверяем двунаправленные связи\n",
        "        if student_spec in related_specs and required_spec in related_specs[student_spec]:\n",
        "            return 0.7\n",
        "\n",
        "        if required_spec in related_specs and student_spec in related_specs[required_spec]:\n",
        "            return 0.7\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def calculate_skill_match(self, student_skills: List[str], topic_keywords: List[str]) -> float:\n",
        "        if not student_skills:\n",
        "            return 0.0\n",
        "\n",
        "        # Преобразуем в множества для быстрого поиска\n",
        "        student_skills_set = set(student_skills)\n",
        "        topic_keywords_set = set(topic_keywords)\n",
        "\n",
        "        # Ищем пересечения навыков и ключевых слов\n",
        "        matches = len([skill for skill in student_skills_set\n",
        "                      if any(skill in kw or kw in skill for kw in topic_keywords_set)])\n",
        "\n",
        "        return matches / len(student_skills_set)\n",
        "\n",
        "    def calculate_comprehensive_score(self, semantic_similarity: float, spec_match: float,\n",
        "                                     skill_match: float, hours_score: float,\n",
        "                                     weights: Optional[Dict] = None) -> float:\n",
        "        if weights is None:\n",
        "            # Веса можно настраивать в зависимости от приоритетов\n",
        "            weights = {'semantic': 0.4, 'specialization': 0.3, 'skills': 0.2, 'hours': 0.1}\n",
        "\n",
        "        # Взвешенная сумма всех метрик\n",
        "        score = (semantic_similarity * weights['semantic'] +\n",
        "                spec_match * weights['specialization'] +\n",
        "                skill_match * weights['skills'] +\n",
        "                hours_score * weights['hours'])\n",
        "\n",
        "        return min(1.0, score)  # Ограничиваем максимальным значением 1.0\n",
        "\n",
        "    def preprocess_student_data(self, student_data: pd.DataFrame) -> Tuple[pd.DataFrame, List[str], List[str]]:\n",
        "        df = student_data.copy()\n",
        "\n",
        "        # Определяем имена столбцов автоматически\n",
        "        col_names = df.columns.tolist()\n",
        "\n",
        "        # Поиск ID студента\n",
        "        id_col = next((col for col in col_names if 'id' in col.lower()),\n",
        "                     next((col for col in col_names if 'номер' in col.lower()), col_names[0]))\n",
        "\n",
        "        # Удаляем строки без ID\n",
        "        df = df.dropna(subset=[id_col])\n",
        "\n",
        "        # Конвертируем ID в числовой формат\n",
        "        df['student_id_int'] = df[id_col].apply(self._safe_convert_to_int)\n",
        "\n",
        "        # Поиск нужных столбцов по ключевым словам\n",
        "        spec_col = next((col for col in col_names if 'специал' in col.lower() or 'роль' in col.lower()),\n",
        "                       next((col for col in col_names if 'special' in col.lower() or 'role' in col.lower()),\n",
        "                           col_names[1] if len(col_names) > 1 else id_col))\n",
        "\n",
        "        exp_col = next((col for col in col_names if 'опыт' in col.lower() or 'experience' in col.lower()),\n",
        "                      col_names[2] if len(col_names) > 2 else spec_col)\n",
        "\n",
        "        int_col = next((col for col in col_names if 'интерес' in col.lower() or 'interest' in col.lower()),\n",
        "                      col_names[3] if len(col_names) > 3 else exp_col)\n",
        "\n",
        "        hours_col = next((col for col in col_names if any(word in col.lower() for word in ['врем', 'час', 'time', 'готовы', 'тратить'])),\n",
        "                        col_names[4] if len(col_names) > 4 else int_col)\n",
        "\n",
        "        # Применяем нормализацию и извлечение признаков\n",
        "        df['specialization_clean'] = df[spec_col].apply(self._normalize_specialization)\n",
        "        df['skills'] = df[exp_col].apply(self._extract_skills)\n",
        "        df['hours_normalized'] = df[hours_col].apply(self._normalize_hours)\n",
        "\n",
        "        # Создаем список текстовых представлений для семантического анализа\n",
        "        student_texts = []\n",
        "        for _, row in df.iterrows():\n",
        "            # Собираем все релевантные текстовые поля\n",
        "            text_parts = []\n",
        "            for col in [spec_col, exp_col, int_col]:\n",
        "                if col in row and pd.notna(row[col]):\n",
        "                    text_parts.append(str(row[col]))\n",
        "            student_texts.append(\" \".join(text_parts))\n",
        "\n",
        "        return df, student_texts, [id_col, spec_col, exp_col, int_col, hours_col]\n",
        "\n",
        "    def _safe_convert_to_int(self, value: Any) -> int:\n",
        "        try:\n",
        "            if pd.isna(value):\n",
        "                return 0\n",
        "\n",
        "            value_str = str(value).strip()\n",
        "            numbers = re.findall(r'\\d+', value_str)\n",
        "\n",
        "            if numbers:\n",
        "                return int(numbers[0])\n",
        "\n",
        "            # Пробуем преобразовать как число с плавающей точкой\n",
        "            return int(float(value))\n",
        "        except (ValueError, TypeError):\n",
        "            return 0\n",
        "\n",
        "    def preprocess_topic_data(self, topic_data: pd.Series) -> Dict[str, Any]:\n",
        "        processed_topic = topic_data.to_dict()\n",
        "\n",
        "        # Определяем, какой столбец содержит специализации\n",
        "        spec_col = None\n",
        "        for col in ['required_specializations', 'Желаемая командная роль', 'роль', 'специализация']:\n",
        "            if col in processed_topic:\n",
        "                spec_col = col\n",
        "                break\n",
        "\n",
        "        if spec_col and pd.notna(processed_topic[spec_col]):\n",
        "            # Разделяем специализации по запятым или другим разделителям\n",
        "            spec_text = str(processed_topic[spec_col])\n",
        "            # Разделяем по запятым, точкам с запятой или переводам строк\n",
        "            required_specs = []\n",
        "            for part in re.split(r'[,;\\n]', spec_text):\n",
        "                spec_clean = part.strip()\n",
        "                if spec_clean:\n",
        "                    required_specs.append(spec_clean)\n",
        "            processed_topic['required_specializations_list'] = required_specs\n",
        "        else:\n",
        "            processed_topic['required_specializations_list'] = []\n",
        "\n",
        "        # Определяем столбец с описанием темы\n",
        "        desc_col = None\n",
        "        for col in ['description', 'Выберите тематику', 'тематика', 'описание']:\n",
        "            if col in processed_topic:\n",
        "                desc_col = col\n",
        "                break\n",
        "\n",
        "        if desc_col:\n",
        "            # Создаем полное текстовое описание\n",
        "            text_parts = []\n",
        "            if desc_col in processed_topic and pd.notna(processed_topic[desc_col]):\n",
        "                text_parts.append(str(processed_topic[desc_col]))\n",
        "\n",
        "            # Добавляем другие релевантные колонки\n",
        "            for col in ['title', 'задача', 'goal', 'tasks', 'goals']:\n",
        "                if col in processed_topic and pd.notna(processed_topic[col]):\n",
        "                    text_parts.append(str(processed_topic[col]))\n",
        "\n",
        "            processed_topic['full_description'] = \" \".join(text_parts)\n",
        "            processed_topic['keywords'] = self._extract_keywords(pd.Series(processed_topic))\n",
        "        else:\n",
        "            processed_topic['full_description'] = \"\"\n",
        "            processed_topic['keywords'] = []\n",
        "\n",
        "        return processed_topic\n",
        "\n",
        "    def find_best_students_for_specialization(self, students_data: pd.DataFrame,\n",
        "                                             topic_data: pd.Series,\n",
        "                                             required_spec: str,\n",
        "                                             top_k: int = 5) -> List[Dict]:\n",
        "        # Предобработка данных\n",
        "        students_df, student_texts, cols = self.preprocess_student_data(students_data)\n",
        "        id_col, spec_col, exp_col, int_col, hours_col = cols\n",
        "\n",
        "        processed_topic = self.preprocess_topic_data(topic_data)\n",
        "\n",
        "        # Вычисление семантической схожести\n",
        "        semantic_scores = self.calculate_semantic_similarity(\n",
        "            student_texts,\n",
        "            processed_topic['full_description']\n",
        "        )\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Оценка каждого студента\n",
        "        for i, (_, student) in enumerate(students_df.iterrows()):\n",
        "            # Соответствие специализации\n",
        "            spec_match = self.calculate_specialization_match(\n",
        "                student['specialization_clean'],\n",
        "                self._normalize_specialization(required_spec)\n",
        "            )\n",
        "\n",
        "            # Соответствие навыков\n",
        "            skill_match = self.calculate_skill_match(\n",
        "                student['skills'],\n",
        "                processed_topic['keywords']\n",
        "            )\n",
        "\n",
        "            # Комплексная оценка\n",
        "            comprehensive_score = self.calculate_comprehensive_score(\n",
        "                semantic_similarity=semantic_scores[i],\n",
        "                spec_match=spec_match,\n",
        "                skill_match=skill_match,\n",
        "                hours_score=student['hours_normalized']\n",
        "            )\n",
        "\n",
        "            # Преобразуем student_id в целое число\n",
        "            raw_student_id = student[id_col]\n",
        "            try:\n",
        "                # Преобразуем в целое число, если это возможно\n",
        "                if isinstance(raw_student_id, (int, np.integer)):\n",
        "                    student_id_int = int(raw_student_id)\n",
        "                elif isinstance(raw_student_id, (float, np.floating)):\n",
        "                    student_id_int = int(raw_student_id)\n",
        "                else:\n",
        "                    # Для строк пытаемся извлечь число\n",
        "                    numbers = re.findall(r'\\d+', str(raw_student_id))\n",
        "                    student_id_int = int(numbers[0]) if numbers else int(student['student_id_int'])\n",
        "            except (ValueError, TypeError, IndexError):\n",
        "                # Если не удается преобразовать, используем внутренний ID\n",
        "                student_id_int = int(student['student_id_int'])\n",
        "\n",
        "            # Сохраняем результат\n",
        "            result = {\n",
        "                'student_id': student_id_int,\n",
        "                'student_specialization': student[spec_col],\n",
        "                'normalized_specialization': student['specialization_clean'],\n",
        "                'required_specialization': required_spec,\n",
        "                'comprehensive_score': float(comprehensive_score),\n",
        "                'semantic_similarity': float(semantic_scores[i]),\n",
        "                'specialization_match': float(spec_match),\n",
        "                'skills_match': float(skill_match),\n",
        "                'available_hours': float(student['hours_normalized']),\n",
        "                'interest': str(student[int_col]) if int_col in student else '',\n",
        "                'experience': str(student[exp_col]) if exp_col in student else ''\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        # Сортировка по комплексной оценке\n",
        "        results.sort(key=lambda x: x['comprehensive_score'], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "    def create_topic_specialization_map(self, students_data: pd.DataFrame,\n",
        "                                       topics_data: pd.DataFrame,\n",
        "                                       top_k_per_spec: int = 5) -> Dict:\n",
        "        topic_specialization_map = {}\n",
        "\n",
        "        # Обрабатываем каждую тему\n",
        "        for topic_idx, topic in topics_data.iterrows():\n",
        "            # Извлекаем или генерируем ID темы\n",
        "            topic_id = topic.get('topic_id',\n",
        "                               topic.get('Id',\n",
        "                                       topic.get('id', f'topic_{topic_idx + 1}')))\n",
        "            topic_id_int = self._safe_convert_to_int(topic_id)\n",
        "\n",
        "            # Определяем название темы\n",
        "            title_col = None\n",
        "            for col in ['title', 'Выберите тематику', 'тематика', 'название']:\n",
        "                if col in topic and pd.notna(topic[col]):\n",
        "                    title_col = col\n",
        "                    break\n",
        "\n",
        "            topic_title = topic[title_col] if title_col else f'Тема {topic_idx + 1}'\n",
        "\n",
        "            # Определяем требуемые специализации\n",
        "            processed_topic = self.preprocess_topic_data(topic)\n",
        "            required_specs = processed_topic['required_specializations_list']\n",
        "\n",
        "            candidates_by_specialization = {}\n",
        "\n",
        "            # Ищем лучших кандидатов для каждой требуемой специализации\n",
        "            for required_spec in required_specs:\n",
        "                if required_spec:  # Пропускаем пустые специализации\n",
        "                    best_students = self.find_best_students_for_specialization(\n",
        "                        students_data, topic, required_spec, top_k=top_k_per_spec\n",
        "                    )\n",
        "\n",
        "                    # Сохраняем только ID студентов\n",
        "                    student_ids = [student['student_id'] for student in best_students]\n",
        "                    candidates_by_specialization[required_spec] = student_ids\n",
        "\n",
        "            # Сохраняем результаты для этой темы\n",
        "            topic_specialization_map[topic_id_int] = {\n",
        "                'title': str(topic_title),\n",
        "                'candidates_by_specialization': candidates_by_specialization\n",
        "            }\n",
        "\n",
        "        return topic_specialization_map\n",
        "\n",
        "    def get_formatted_output(self, topic_specialization_map: Dict) -> Dict:\n",
        "        formatted_output = {}\n",
        "\n",
        "        for topic_id, topic_info in topic_specialization_map.items():\n",
        "            project_key = f\"Проект{topic_id}\"\n",
        "            formatted_output[project_key] = topic_info['candidates_by_specialization']\n",
        "\n",
        "        return formatted_output\n",
        "\n",
        "    def save_results_to_json(self, topic_specialization_map: Dict,\n",
        "                            filename: str = \"topic_candidate_map.json\") -> Dict:\n",
        "        formatted_data = self.get_formatted_output(topic_specialization_map)\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        print(f\"Результаты сохранены в файл: {filename}\")\n",
        "        return formatted_data\n",
        "\n",
        "# Создаем экземпляр модели\n",
        "print(\"Инициализация модели...\")\n",
        "matcher = CSVStudentTopicMatcher()\n",
        "print(\"Модель загружена и готова к работе!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WFiQfsyqFd9e",
        "outputId": "7d0888d8-d3ef-4e05-ce83-319f7093c56f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Инициализация модели...\n",
            "Модель загружена и готова к работе!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def detect_environment():\n",
        "    try:\n",
        "        # Проверяем, находимся ли мы в Google Colab\n",
        "        import google.colab\n",
        "        return \"colab\"\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    try:\n",
        "        # Проверяем, находимся ли мы в Jupyter\n",
        "        from IPython import get_ipython\n",
        "        if get_ipython() is not None:\n",
        "            return \"jupyter\"\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    return \"vscode\"  # По умолчанию предполагаем VS Code\n",
        "\n",
        "def upload_file_colab():\n",
        "    from google.colab import files\n",
        "    print(\"Пожалуйста, загрузите файл...\")\n",
        "    uploaded = files.upload()\n",
        "\n",
        "    if uploaded:\n",
        "        filename = list(uploaded.keys())[0]\n",
        "        print(f\"Файл '{filename}' успешно загружен!\")\n",
        "        return filename\n",
        "    return None\n",
        "\n",
        "def ask_for_file_path(filename_hint=\"\"):\n",
        "    default_paths = [\n",
        "        filename_hint,\n",
        "        f\"./{filename_hint}\",\n",
        "        f\"../{filename_hint}\",\n",
        "        f\"data/{filename_hint}\",\n",
        "        f\"../data/{filename_hint}\",\n",
        "    ]\n",
        "\n",
        "    print(f\"Файл '{filename_hint}' не найден по умолчанию.\")\n",
        "    print(\"Возможные варианты:\")\n",
        "    for i, path in enumerate(default_paths, 1):\n",
        "        if path and path != filename_hint:\n",
        "            print(f\"  {i}. {path}\")\n",
        "\n",
        "    print(f\"\\nВведите путь к файлу (или оставьте пустым для '{filename_hint}'):\")\n",
        "    user_path = input().strip()\n",
        "\n",
        "    if not user_path:\n",
        "        return filename_hint\n",
        "    return user_path\n",
        "\n",
        "# Определяем среду\n",
        "environment = detect_environment()\n",
        "print(f\"Определена среда: {environment.upper()}\")\n",
        "\n",
        "# ============================================\n",
        "# ЗАГРУЗКА ДАННЫХ СТУДЕНТОВ\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ЗАГРУЗКА ДАННЫХ СТУДЕНТОВ\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "students_loaded = False\n",
        "student_file_options = [\n",
        "    'Students_LISA.xlsx',\n",
        "    'students.xlsx',\n",
        "    'students.xls',\n",
        "    'Students.xlsx',\n",
        "    'students_data.xlsx',\n",
        "    'LISA_students.xlsx'\n",
        "]\n",
        "\n",
        "# Пробуем найти файл автоматически\n",
        "for filename in student_file_options:\n",
        "    try:\n",
        "        if environment == \"colab\":\n",
        "            # В Colab пробуем загрузить из файловой системы\n",
        "            students_df = pd.read_excel(filename)\n",
        "            print(f\"✓ Файл студентов найден: {filename}\")\n",
        "            print(f\"  Загружено {len(students_df)} записей\")\n",
        "            print(f\"  Столбцы: {students_df.columns.tolist()}\")\n",
        "            students_loaded = True\n",
        "            break\n",
        "        else:\n",
        "            # В Jupyter/VS Code пробуем стандартные пути\n",
        "            if os.path.exists(filename):\n",
        "                students_df = pd.read_excel(filename)\n",
        "                print(f\"✓ Файл студентов найден: {filename}\")\n",
        "                print(f\"  Загружено {len(students_df)} записей\")\n",
        "                print(f\"  Столбцы: {students_df.columns.tolist()}\")\n",
        "                students_loaded = True\n",
        "                break\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "# Если файл не найден автоматически\n",
        "if not students_loaded:\n",
        "    print(\"\\nФайл со студентами не найден автоматически.\")\n",
        "\n",
        "    if environment == \"colab\":\n",
        "        print(\"Запускаю загрузку файла студентов...\")\n",
        "        student_filename = upload_file_colab()\n",
        "        if student_filename:\n",
        "            students_df = pd.read_excel(student_filename)\n",
        "            students_loaded = True\n",
        "    else:\n",
        "        # Для Jupyter/VS Code запрашиваем путь\n",
        "        student_filename = ask_for_file_path(\"Students_LISA.xlsx\")\n",
        "        try:\n",
        "            students_df = pd.read_excel(student_filename)\n",
        "            students_loaded = True\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка загрузки: {e}\")\n",
        "\n",
        "if not students_loaded:\n",
        "    raise FileNotFoundError(\"Не удалось загрузить данные студентов\")\n",
        "\n",
        "# ============================================\n",
        "# ЗАГРУЗКА ДАННЫХ ТЕМ ПРОЕКТОВ\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ЗАГРУЗКА ДАННЫХ ТЕМ ПРОЕКТОВ\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "topics_loaded = False\n",
        "topic_file_options = [\n",
        "    'topics_Lisa.xlsx',\n",
        "    'topics_LISA.xlsx',\n",
        "    'topics.xlsx',\n",
        "    'topics.xls',\n",
        "    'Topics.xlsx',\n",
        "    'topics_data.xlsx',\n",
        "    'Lisa_topics.xlsx'\n",
        "]\n",
        "\n",
        "# Пробуем найти файл автоматически\n",
        "for filename in topic_file_options:\n",
        "    try:\n",
        "        if environment == \"colab\":\n",
        "            # В Colab пробуем загрузить из файловой системы\n",
        "            topics_df = pd.read_excel(filename)\n",
        "            print(f\"✓ Файл тем найден: {filename}\")\n",
        "            print(f\"  Загружено {len(topics_df)} записей\")\n",
        "            print(f\"  Столбцы: {topics_df.columns.tolist()}\")\n",
        "            topics_loaded = True\n",
        "            break\n",
        "        else:\n",
        "            # В Jupyter/VS Code пробуем стандартные пути\n",
        "            if os.path.exists(filename):\n",
        "                topics_df = pd.read_excel(filename)\n",
        "                print(f\"✓ Файл тем найден: {filename}\")\n",
        "                print(f\"  Загружено {len(topics_df)} записей\")\n",
        "                print(f\"  Столбцы: {topics_df.columns.tolist()}\")\n",
        "                topics_loaded = True\n",
        "                break\n",
        "    except Exception as e:\n",
        "        continue\n",
        "\n",
        "# Если файл не найден автоматически\n",
        "if not topics_loaded:\n",
        "    print(\"\\nФайл с темами не найден автоматически.\")\n",
        "\n",
        "    if environment == \"colab\":\n",
        "        print(\"Запускаю загрузку файла тем...\")\n",
        "        topic_filename = upload_file_colab()\n",
        "        if topic_filename:\n",
        "            topics_df = pd.read_excel(topic_filename)\n",
        "            topics_loaded = True\n",
        "    else:\n",
        "        # запрашиваем путь\n",
        "        topic_filename = ask_for_file_path(\"topics_Lisa.xlsx\")\n",
        "        try:\n",
        "            topics_df = pd.read_excel(topic_filename)\n",
        "            topics_loaded = True\n",
        "        except Exception as e:\n",
        "            print(f\"Ошибка загрузки: {e}\")\n",
        "\n",
        "if not topics_loaded:\n",
        "    raise FileNotFoundError(\"Не удалось загрузить данные тем проектов\")\n",
        "\n",
        "# ============================================\n",
        "# ПРЕДВАРИТЕЛЬНЫЙ ПРОСМОТР ДАННЫХ\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ПРЕДВАРИТЕЛЬНЫЙ ПРОСМОТР ДАННЫХ\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"\\n--- СТУДЕНТЫ (первые 3 записи) ---\")\n",
        "print(students_df.head(3))\n",
        "print(f\"\\nВсего студентов: {len(students_df)}\")\n",
        "\n",
        "print(\"\\n--- ТЕМЫ ПРОЕКТОВ (первые 3 записи) ---\")\n",
        "print(topics_df.head(3))\n",
        "print(f\"\\nВсего тем: {len(topics_df)}\")\n",
        "\n",
        "# ============================================\n",
        "# ВЫПОЛНЕНИЕ СОПОСТАВЛЕНИЯ\n",
        "# ============================================\n",
        "\n",
        "print(\"\\n\" + \"=\"*50)\n",
        "print(\"ВЫПОЛНЕНИЕ СОПОСТАВЛЕНИЯ\")\n",
        "print(\"=\"*50)\n",
        "\n",
        "print(\"Это может занять некоторое время...\")\n",
        "\n",
        "try:\n",
        "    topic_specialization_map = matcher.create_topic_specialization_map(\n",
        "        students_df,\n",
        "        topics_df,\n",
        "        top_k_per_spec=5\n",
        "    )\n",
        "\n",
        "    print(\"✓ Сопоставление успешно завершено!\")\n",
        "\n",
        "    final_data = matcher.save_results_to_json(topic_specialization_map)\n",
        "\n",
        "    # ============================================\n",
        "    # ВЫВОД РЕЗУЛЬТАТОВ\n",
        "    # ============================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"РЕЗУЛЬТАТЫ СОПОСТАВЛЕНИЯ\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    total_matches = 0\n",
        "    for topic_id, topic_info in topic_specialization_map.items():\n",
        "        candidates = topic_info['candidates_by_specialization']\n",
        "        num_candidates = sum(len(ids) for ids in candidates.values())\n",
        "        if num_candidates > 0:\n",
        "            print(f\"\\nТема #{topic_id}: {topic_info['title'][:50]}...\")\n",
        "            total_matches += num_candidates\n",
        "\n",
        "            for spec, student_ids in candidates.items():\n",
        "                if student_ids:\n",
        "                    print(f\"  {spec}: {student_ids}\")\n",
        "\n",
        "    print(f\"\\nВсего найдено сопоставлений: {total_matches}\")\n",
        "\n",
        "    # ============================================\n",
        "    # СОХРАНЕНИЕ И СКАЧИВАНИЕ РЕЗУЛЬТАТОВ\n",
        "    # ============================================\n",
        "\n",
        "    if environment == \"colab\":\n",
        "        try:\n",
        "            from google.colab import files\n",
        "            files.download(\"topic_candidate_map.json\")\n",
        "            print(\"\\n✓ Файл с результатами скачан на ваше устройство!\")\n",
        "        except:\n",
        "            print(\"\\n✓ Файл сохранен: topic_candidate_map.json\")\n",
        "    else:\n",
        "        # Для Jupyter/VS Code предлагаем открыть файл\n",
        "        print(\"\\n✓ Результаты сохранены в файл: topic_candidate_map.json\")\n",
        "        print(\"Файл находится в текущей директории:\")\n",
        "        print(f\"  {os.path.abspath('topic_candidate_map.json')}\")\n",
        "\n",
        "        # Проверяем существование файла\n",
        "        if os.path.exists(\"topic_candidate_map.json\"):\n",
        "            file_size = os.path.getsize(\"topic_candidate_map.json\")\n",
        "            print(f\"  Размер файла: {file_size:,} байт\")\n",
        "\n",
        "            # Показываем первые несколько строк для чекапа\n",
        "            try:\n",
        "                with open(\"topic_candidate_map.json\", 'r', encoding='utf-8') as f:\n",
        "                    first_lines = [next(f) for _ in range(5)]\n",
        "                print(\"\\nПервые строки файла:\")\n",
        "                print(\"\".join(first_lines))\n",
        "            except:\n",
        "                pass\n",
        "\n",
        "    # ============================================\n",
        "    # ДОПОЛНИТЕЛЬНЫЕ ВОЗМОЖНОСТИ\n",
        "    # ============================================\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"ДОПОЛНИТЕЛЬНЫЕ ВОЗМОЖНОСТИ\")\n",
        "    print(\"=\"*50)\n",
        "    print(\"Доступные переменные:\")\n",
        "    print(\"1. students_df - данные студентов\")\n",
        "    print(\"2. topics_df - данные тем проектов\")\n",
        "    print(\"3. final_data - результаты в формате словаря\")\n",
        "    print(\"4. topic_specialization_map - полные результаты с деталями\")\n",
        "    print(\"\\nПример использования:\")\n",
        "    print(\"  final_data['Проект1'] - получить кандидатов для проекта 1\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"\\n✗ Ошибка при сопоставлении: {e}\")\n",
        "    print(\"Проверьте структуру данных и попробуйте снова.\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "bzOez1RMGePZ",
        "outputId": "e7dd3757-a6ab-409e-fd13-993a16830a10"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Определена среда: COLAB\n",
            "\n",
            "==================================================\n",
            "ЗАГРУЗКА ДАННЫХ СТУДЕНТОВ\n",
            "==================================================\n",
            "✓ Файл студентов найден: Students_LISA.xlsx\n",
            "  Загружено 82 записей\n",
            "  Столбцы: ['Номер', 'Роль', 'Интерес', 'Коммерческий опыт', 'Сколько вы готовы тратить часов в неделю на  работу в лаборатории LISA?']\n",
            "\n",
            "==================================================\n",
            "ЗАГРУЗКА ДАННЫХ ТЕМ ПРОЕКТОВ\n",
            "==================================================\n",
            "✓ Файл тем найден: topics_LISA.xlsx\n",
            "  Загружено 13 записей\n",
            "  Столбцы: ['Id', 'Выберите тематику', 'Желаемая командная роль']\n",
            "\n",
            "==================================================\n",
            "ПРЕДВАРИТЕЛЬНЫЙ ПРОСМОТР ДАННЫХ\n",
            "==================================================\n",
            "\n",
            "--- СТУДЕНТЫ (первые 3 записи) ---\n",
            "   Номер               Роль                                    Интерес  \\\n",
            "0      1  Backend Developer                         Fullstack и DevOps   \n",
            "1      2                 РП                            ИИ в госсекторе   \n",
            "2      3        ML Engineer  Машинное обучение, LLM, бекенд-разработка   \n",
            "\n",
            "                                   Коммерческий опыт  \\\n",
            "0  Фриланс Fullstack  время Nestjs, Nextjs, Linux...   \n",
            "1                                 15 лет опыта в ИТ    \n",
            "2  АНБ  python, FastAPI\\nДата Аквилон  время pyth...   \n",
            "\n",
            "  Сколько вы готовы тратить часов в неделю на  работу в лаборатории LISA?  \n",
            "0                                           15-20 ч.                       \n",
            "1                                            до 5 ч.                       \n",
            "2                                           10-15 ч.                       \n",
            "\n",
            "Всего студентов: 82\n",
            "\n",
            "--- ТЕМЫ ПРОЕКТОВ (первые 3 записи) ---\n",
            "   Id                                  Выберите тематику  \\\n",
            "0   1            Платформа управления учебными проектами   \n",
            "1   2       Организационная и инициативная работа в LISA   \n",
            "2   3  Исследование и разработка методов context engi...   \n",
            "\n",
            "                             Желаемая командная роль  \n",
            "0      Backend Developer, Frontend Developer, DevOps  \n",
            "1                     ML Engineer, Backend Developer  \n",
            "2  ML Engineer, Backend Developer, Frontend Devel...  \n",
            "\n",
            "Всего тем: 13\n",
            "\n",
            "==================================================\n",
            "ВЫПОЛНЕНИЕ СОПОСТАВЛЕНИЯ\n",
            "==================================================\n",
            "Это может занять некоторое время...\n",
            "✓ Сопоставление успешно завершено!\n",
            "Результаты сохранены в файл: topic_candidate_map.json\n",
            "\n",
            "==================================================\n",
            "РЕЗУЛЬТАТЫ СОПОСТАВЛЕНИЯ\n",
            "==================================================\n",
            "\n",
            "Тема #1: Платформа управления учебными проектами...\n",
            "  Backend Developer: [30, 53, 16, 23, 45]\n",
            "  Frontend Developer: [43, 42, 12, 74, 61]\n",
            "  DevOps: [30, 20, 53, 16, 54]\n",
            "\n",
            "Тема #2: Организационная и инициативная работа в LISA...\n",
            "  ML Engineer: [17, 8, 64, 68, 72]\n",
            "  Backend Developer: [16, 30, 53, 18, 7]\n",
            "\n",
            "Тема #3: Исследование и разработка методов context engineer...\n",
            "  ML Engineer: [59, 35, 72, 63, 68]\n",
            "  Backend Developer: [23, 30, 53, 26, 39]\n",
            "  Frontend Developer: [42, 43, 12, 74, 61]\n",
            "\n",
            "Тема #4: Повышение производительности обучения БЯМ на класт...\n",
            "  ML Engineer: [57, 76, 66, 19, 81]\n",
            "  Data Engineer: [53, 18, 22, 1, 39]\n",
            "  Backend Developer: [53, 18, 22, 1, 39]\n",
            "  DevOps: [24, 54, 20, 53, 18]\n",
            "\n",
            "Тема #5: Edulytica...\n",
            "  Backend Developer: [69, 53, 30, 27, 22]\n",
            "  DevOps: [20, 24, 69, 54, 53]\n",
            "  ML Engineer: [81, 6, 64, 68, 63]\n",
            "\n",
            "Тема #6: Разработка мультиагентной системы для поддержки пр...\n",
            "  ML Engineer: [80, 13, 38, 71, 15]\n",
            "  ML Researcher: [80, 13, 38, 71, 15]\n",
            "  Backend Developer: [7, 53, 30, 16, 18]\n",
            "  DevOps: [20, 54, 24, 7, 53]\n",
            "\n",
            "Тема #7: Платформа непрерывного образования...\n",
            "  ML Engineer: [72, 75, 68, 55, 38]\n",
            "  Backend Developer: [30, 53, 16, 7, 23]\n",
            "  Frontend Developer: [43, 42, 74, 12, 61]\n",
            "\n",
            "Тема #8: Нейросеть для обработки титульных листов выпускных...\n",
            "  ML Engineer: [79, 59, 60, 64, 35]\n",
            "  Backend Developer: [30, 53, 45, 39, 16]\n",
            "  Frontend Developer: [43, 74, 12, 42, 61]\n",
            "  DevOps: [30, 24, 20, 53, 54]\n",
            "\n",
            "Тема #9: Разработка гибридного LLM-агента для анализа кибер...\n",
            "  ML Engineer: [13, 38, 46, 80, 31]\n",
            "  Data Analyst: [14, 56, 67, 40, 2]\n",
            "  Data Engineer: [7, 24, 53, 18, 69]\n",
            "\n",
            "Тема #10: Разработка симулятора для multi-agent reinforcemen...\n",
            "  ML Engineer: [55, 81, 76, 28, 13]\n",
            "  Backend Developer: [53, 39, 7, 23, 18]\n",
            "  DevOps: [24, 20, 54, 53, 39]\n",
            "\n",
            "Тема #11: Разработка сервиса для оценки когнитивных функций ...\n",
            "  ML Engineer: [66, 70, 37, 11, 59]\n",
            "  Data Engineer: [53, 69, 23, 45, 30]\n",
            "\n",
            "Тема #12: Разработка квантовоподобных подходов к моделирован...\n",
            "  ML Engineer: [81, 76, 68, 66, 19]\n",
            "  AI PM: [81, 76, 68, 66, 19]\n",
            "\n",
            "Тема #13: Чат-бот для помощи выбора тем ВКР и подбора студен...\n",
            "  ML Engineer: [13, 11, 75, 60, 70]\n",
            "  Backend Developer: [30, 53, 16, 39, 18]\n",
            "  Frontend Developer: [43, 12, 42, 74, 61]\n",
            "  DevOps: [20, 54, 24, 30, 53]\n",
            "\n",
            "Всего найдено сопоставлений: 200\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "\n",
              "    async function download(id, filename, size) {\n",
              "      if (!google.colab.kernel.accessAllowed) {\n",
              "        return;\n",
              "      }\n",
              "      const div = document.createElement('div');\n",
              "      const label = document.createElement('label');\n",
              "      label.textContent = `Downloading \"${filename}\": `;\n",
              "      div.appendChild(label);\n",
              "      const progress = document.createElement('progress');\n",
              "      progress.max = size;\n",
              "      div.appendChild(progress);\n",
              "      document.body.appendChild(div);\n",
              "\n",
              "      const buffers = [];\n",
              "      let downloaded = 0;\n",
              "\n",
              "      const channel = await google.colab.kernel.comms.open(id);\n",
              "      // Send a message to notify the kernel that we're ready.\n",
              "      channel.send({})\n",
              "\n",
              "      for await (const message of channel.messages) {\n",
              "        // Send a message to notify the kernel that we're ready.\n",
              "        channel.send({})\n",
              "        if (message.buffers) {\n",
              "          for (const buffer of message.buffers) {\n",
              "            buffers.push(buffer);\n",
              "            downloaded += buffer.byteLength;\n",
              "            progress.value = downloaded;\n",
              "          }\n",
              "        }\n",
              "      }\n",
              "      const blob = new Blob(buffers, {type: 'application/binary'});\n",
              "      const a = document.createElement('a');\n",
              "      a.href = window.URL.createObjectURL(blob);\n",
              "      a.download = filename;\n",
              "      div.appendChild(a);\n",
              "      a.click();\n",
              "      div.remove();\n",
              "    }\n",
              "  "
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ],
            "application/javascript": [
              "download(\"download_f7fe3d9f-812e-4249-bbdf-69c99ada80d4\", \"topic_candidate_map.json\", 3463)"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "✓ Файл с результатами скачан на ваше устройство!\n",
            "\n",
            "==================================================\n",
            "ДОПОЛНИТЕЛЬНЫЕ ВОЗМОЖНОСТИ\n",
            "==================================================\n",
            "Доступные переменные:\n",
            "1. students_df - данные студентов\n",
            "2. topics_df - данные тем проектов\n",
            "3. final_data - результаты в формате словаря\n",
            "4. topic_specialization_map - полные результаты с деталями\n",
            "\n",
            "Пример использования:\n",
            "  final_data['Проект1'] - получить кандидатов для проекта 1\n"
          ]
        }
      ]
    }
  ]
}