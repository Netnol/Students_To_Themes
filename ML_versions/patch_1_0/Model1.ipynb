{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install sentence-transformers transformers torch scikit-learn pandas numpy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yFy5er2eHId3"
      },
      "outputs": [],
      "source": [
        "\"\"\"Важное замечание: данный загрузчик предназначен только для jupyter notebook,\n",
        " не для VS Code, поэтому для просмотра полной реализации патча 1.0 посмотрите\n",
        " здешний .ipynb\"\"\"\n",
        "\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "import re\n",
        "import chardet\n",
        "from google.colab import files\n",
        "import io\n",
        "\n",
        "#Автоматически определяет кодировку и разделитель файла\n",
        "def detect_file_params(file_content):\n",
        "\n",
        "    encoding_result = chardet.detect(file_content)\n",
        "    encoding = encoding_result['encoding']\n",
        "    confidence = encoding_result['confidence']\n",
        "\n",
        "\n",
        "    sample_size = min(10000, len(file_content))\n",
        "    sample = file_content[:sample_size]\n",
        "\n",
        "    try:\n",
        "        decoded_sample = sample.decode(encoding)\n",
        "    except:\n",
        "\n",
        "        decoded_sample = sample.decode('utf-8', errors='ignore')\n",
        "\n",
        "    separators = [',', ';', '\\t', '|']\n",
        "    separator_scores = {}\n",
        "\n",
        "    for sep in separators:\n",
        "        first_line = decoded_sample.split('\\n')[0] if '\\n' in decoded_sample else decoded_sample\n",
        "        sep_count = first_line.count(sep)\n",
        "        separator_scores[sep] = sep_count\n",
        "\n",
        "    best_separator = max(separator_scores, key=separator_scores.get)\n",
        "\n",
        "\n",
        "    return encoding, best_separator\n",
        "\n",
        "#загрузка\n",
        "def load_csv_automatic(file_content, file_type):\n",
        "    try:\n",
        "        encoding, separator = detect_file_params(file_content)\n",
        "\n",
        "        df = pd.read_csv(\n",
        "            io.BytesIO(file_content),\n",
        "            sep=separator,\n",
        "            encoding=encoding,\n",
        "            on_bad_lines='skip'\n",
        "        )\n",
        "        return df\n",
        "\n",
        "    except Exception as e:\n",
        "\n",
        "        encodings = ['utf-8', 'cp1251', 'windows-1251', 'iso-8859-1', 'koi8-r', 'mac_cyrillic']\n",
        "        separators = [',', ';', '\\t', '|']\n",
        "\n",
        "        for encoding in encodings:\n",
        "            for separator in separators:\n",
        "                try:\n",
        "                    df = pd.read_csv(\n",
        "                        io.BytesIO(file_content),\n",
        "                        sep=separator,\n",
        "                        encoding=encoding,\n",
        "                        on_bad_lines='skip'\n",
        "                    )\n",
        "                    if len(df) > 0 and len(df.columns) > 1:\n",
        "                        print(f\" {file_type} загружен с {encoding}, разделитель '{separator}'\")\n",
        "                        return df\n",
        "                except:\n",
        "                    continue\n",
        "\n",
        "        print(f\" Не удалось загрузить {file_type} ни с одной комбинацией\")\n",
        "        return None\n",
        "\n",
        "def validate_dataframes(students_df, topics_df):\n",
        "    issues = []\n",
        "\n",
        "\n",
        "    if students_df is not None:\n",
        "        required_student_cols = ['id', 'Специализация', 'Опыт в проектах', 'Интересы', 'Сколько времени в неделю(ч)']\n",
        "        missing_student_cols = [col for col in required_student_cols if col not in students_df.columns]\n",
        "        if missing_student_cols:\n",
        "            issues.append(f\"Отсутствуют колонки студентов: {missing_student_cols}\")\n",
        "\n",
        "    if topics_df is not None:\n",
        "        required_topic_cols = ['topic_id', 'title', 'description', 'num_people', 'required_specializations']\n",
        "        missing_topic_cols = [col for col in required_topic_cols if col not in topics_df.columns]\n",
        "        if missing_topic_cols:\n",
        "            issues.append(f\"Отсутствуют колонки тем: {missing_topic_cols}\")\n",
        "\n",
        "    return issues\n",
        "\n",
        "\n",
        "\n",
        "print(\" Загрузите файл students.csv\")\n",
        "uploaded_students = files.upload()\n",
        "\n",
        "print(\" Загрузите файл topics.csv\")\n",
        "uploaded_topics = files.upload()\n",
        "\n",
        "\n",
        "students_filename = list(uploaded_students.keys())[0] if uploaded_students else None\n",
        "topics_filename = list(uploaded_topics.keys())[0] if uploaded_topics else None\n",
        "\n",
        "\n",
        "students_df = None\n",
        "topics_df = None\n",
        "\n",
        "if students_filename:\n",
        "    students_df = load_csv_automatic(uploaded_students[students_filename], \"students.csv\")\n",
        "\n",
        "if topics_filename:\n",
        "    topics_df = load_csv_automatic(uploaded_topics[topics_filename], \"topics.csv\")\n",
        "\n",
        "\n",
        "print(\"\\n ФИНАЛЬНЫЙ РЕЗУЛЬТАТ ЗАГРУЗКИ:\")\n",
        "print(f\" Студентов: {len(students_df)}\")\n",
        "print(f\" Тем: {len(topics_df)}\")\n",
        "\n",
        "print(\"\\n Структура students:\")\n",
        "print(students_df.info())\n",
        "print(\"\\nПример данных:\")\n",
        "print(students_df.head(3))\n",
        "\n",
        "print(\"\\n Структура topics:\")\n",
        "print(topics_df.info())\n",
        "print(\"\\nПример данных:\")\n",
        "print(topics_df.head(3))\n",
        "\n",
        "\n",
        "print(\"\\n ПРОВЕРКА ДАННЫХ:\")\n",
        "print(f\"Специализации студентов: {students_df['Специализация'].unique()[:10]}\")  # первые 10\n",
        "print(f\"Требуемые специализации в темах: {topics_df['required_specializations'].unique()[:10]}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gugN1AhhHLeE"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import re\n",
        "import json\n",
        "from google.colab import files\n",
        "from typing import List, Dict, Tuple, Optional, Any\n",
        "from functools import lru_cache\n",
        "\n",
        "class CSVStudentTopicMatcher:\n",
        "    \"\"\"Класс для сопоставления студентов и тем проектов на основе семантического анализа\"\"\"\n",
        "\n",
        "    def __init__(self, model_name='sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2'):\n",
        "        # Используем оптимизированную модель для баланса скорости и качества\n",
        "        self.model = SentenceTransformer(model_name)\n",
        "\n",
        "        # Кэш для хранения эмбеддингов студентов (оптимизация производительности)\n",
        "        self._student_embeddings_cache = {}\n",
        "\n",
        "        # Словарь для нормализации специализаций с приоритетом по точному совпадению\n",
        "        self.specialization_mapping = {\n",
        "            'Machine Learning': ['Machine Learning', 'ML', 'AI', 'машинное обучение', 'ml', 'ai', 'machine learning'],\n",
        "            'Data Science': ['Data Science', 'Data Analytics', 'анализ данных', 'data science', 'data analytics'],\n",
        "            'NLP': ['NLP', 'Natural Language Processing', 'обработка текста', 'nlp', 'natural language processing'],\n",
        "            'Computer Vision': ['Computer Vision', 'CV', 'компьютерное зрение', 'computer vision', 'cv'],\n",
        "            'Data Engineering': ['Data Engineering', 'ETL', 'Big Data', 'инженерия данных', 'data engineering'],\n",
        "            'Backend': ['Backend', 'API', 'Microservices', 'Server-side', 'бэкенд', 'backend', 'back-end'],\n",
        "            'Frontend': ['Frontend', 'UI', 'UX', 'Web', 'React', 'Vue', 'фронтенд', 'frontend', 'front-end'],\n",
        "            'Android': ['Android', 'Mobile', 'Kotlin', 'мобильная разработка', 'android', 'mobile development'],\n",
        "            'DevOps': ['DevOps', 'Cloud', 'CI/CD', 'Infrastructure', 'девопс', 'devops'],\n",
        "            'QA': ['QA', 'Testing', 'Test Automation', 'Quality Assurance', 'тестирование', 'qa', 'quality assurance'],\n",
        "            'UI/UX': ['UI/UX', 'Design', 'User Experience', 'Interface', 'дизайн', 'ui/ux', 'ui', 'ux'],\n",
        "            'GameDev': ['GameDev', 'Game Development', 'VR', 'AR', 'геймдев', 'game development'],\n",
        "            'Биоинформатика': ['Биоинформатика', 'Bioinformatics', 'Genomics', 'Biology', 'геномика'],\n",
        "            'Cybersecurity': ['Cybersecurity', 'Security', 'InfoSec', 'кибербезопасность', 'cybersecurity'],\n",
        "            'Robotics': ['Robotics', 'Robots', 'Automation', 'робототехника'],\n",
        "            'Product Analytics': ['Product Analytics', 'Analytics', 'BI', 'Business Intelligence', 'аналитика'],\n",
        "            'Other': ['Other', 'Другое', 'Прочее', 'Разное']\n",
        "        }\n",
        "\n",
        "        # Создаем обратный индекс для быстрого поиска специализаций\n",
        "        self._spec_reverse_index = {}\n",
        "        for main_spec, variants in self.specialization_mapping.items():\n",
        "            for variant in variants:\n",
        "                self._spec_reverse_index[variant.lower()] = main_spec\n",
        "\n",
        "    @lru_cache(maxsize=1000)\n",
        "    def _normalize_specialization(self, specialization: str) -> str:\n",
        "        \"\"\"Нормализация специализации студента к стандартному формату\"\"\"\n",
        "        if pd.isna(specialization):\n",
        "            return 'Other'\n",
        "\n",
        "        spec_str = str(specialization).strip()\n",
        "        if not spec_str or spec_str.lower() in ['nan', 'none', '']:\n",
        "            return 'Other'\n",
        "\n",
        "        spec_lower = spec_str.lower()\n",
        "\n",
        "        # Проверяем точное совпадение в обратном индексе\n",
        "        if spec_lower in self._spec_reverse_index:\n",
        "            return self._spec_reverse_index[spec_lower]\n",
        "\n",
        "        # Проверяем частичное совпадение\n",
        "        for variant, main_spec in self._spec_reverse_index.items():\n",
        "            if variant in spec_lower or spec_lower in variant:\n",
        "                return main_spec\n",
        "\n",
        "        # Маппинг русских названий на английские\n",
        "        ru_en_map = {\n",
        "            'машинное обучение': 'Machine Learning',\n",
        "            'анализ данных': 'Data Science',\n",
        "            'обработка текста': 'NLP',\n",
        "            'компьютерное зрение': 'Computer Vision',\n",
        "            'инженерия данных': 'Data Engineering',\n",
        "            'бэкенд': 'Backend',\n",
        "            'фронтенд': 'Frontend',\n",
        "            'мобильная разработка': 'Android',\n",
        "            'девопс': 'DevOps',\n",
        "            'тестирование': 'QA',\n",
        "            'дизайн': 'UI/UX',\n",
        "            'геймдев': 'GameDev',\n",
        "            'биоинформатика': 'Биоинформатика',\n",
        "            'кибербезопасность': 'Cybersecurity',\n",
        "            'робототехника': 'Robotics',\n",
        "            'аналитика': 'Product Analytics'\n",
        "        }\n",
        "\n",
        "        for ru, en in ru_en_map.items():\n",
        "            if ru in spec_lower:\n",
        "                return en\n",
        "\n",
        "        return 'Other'\n",
        "\n",
        "    def _extract_skills(self, experience: str) -> List[str]:\n",
        "        \"\"\"Извлечение навыков из текста опыта работы\"\"\"\n",
        "        if pd.isna(experience):\n",
        "            return []\n",
        "\n",
        "        experience_str = str(experience)\n",
        "        skills = set()  # Используем set для избежания дубликатов\n",
        "\n",
        "        # Оптимизированный словарь навыков с ключевыми словами\n",
        "        skill_keywords = {\n",
        "            'python': ['python', 'pytorch', 'tensorflow', 'keras', 'pandas', 'numpy', 'scikit-learn', 'sklearn'],\n",
        "            'java': ['java', 'spring', 'hibernate'],\n",
        "            'kotlin': ['kotlin'],\n",
        "            'sql': ['sql', 'postgresql', 'mysql', 'postgres', 'database'],\n",
        "            'javascript': ['javascript', 'js', 'react', 'vue', 'angular', 'typescript', 'node.js', 'nodejs', 'node'],\n",
        "            'docker': ['docker', 'container'],\n",
        "            'kubernetes': ['kubernetes', 'k8s'],\n",
        "            'ml': ['machine learning', 'ml', 'ai', 'нейросети', 'машинное обучение', 'deep learning'],\n",
        "            'nlp': ['nlp', 'natural language', 'текст', 'linguistics'],\n",
        "            'cv': ['computer vision', 'cv', 'image', 'vision', 'opencv', 'компьютерное зрение'],\n",
        "            'data': ['data science', 'data analysis', 'data engineering', 'анализ данных', 'big data'],\n",
        "            'web': ['web', 'frontend', 'backend', 'api', 'веб', 'website', 'fullstack'],\n",
        "            'mobile': ['mobile', 'android', 'ios', 'мобильный', 'react native', 'flutter'],\n",
        "            'devops': ['devops', 'ci/cd', 'cloud', 'aws', 'azure', 'gcp', 'github actions'],\n",
        "            'qa': ['qa', 'testing', 'test', 'quality', 'тестирование', 'selenium', 'automation']\n",
        "        }\n",
        "\n",
        "        experience_lower = experience_str.lower()\n",
        "\n",
        "        # Быстрый поиск навыков по ключевым словам\n",
        "        for skill, keywords in skill_keywords.items():\n",
        "            if any(keyword in experience_lower for keyword in keywords):\n",
        "                skills.add(skill)\n",
        "\n",
        "        return list(skills)\n",
        "\n",
        "    def _normalize_hours(self, hours) -> float:\n",
        "        \"\"\"Нормализация доступного времени студента в числовой коэффициент\"\"\"\n",
        "        try:\n",
        "            # Преобразуем в число, извлекая цифры из строки\n",
        "            if isinstance(hours, str):\n",
        "                numbers = re.findall(r'\\d+', hours)\n",
        "                hours = int(numbers[0]) if numbers else 0\n",
        "            else:\n",
        "                hours = int(float(hours))\n",
        "\n",
        "            # Бинаризация времени на основе квантилей\n",
        "            if hours <= 10:\n",
        "                return 0.3\n",
        "            elif hours <= 15:\n",
        "                return 0.6\n",
        "            elif hours <= 20:\n",
        "                return 0.8\n",
        "            else:\n",
        "                return 1.0\n",
        "        except (ValueError, TypeError):\n",
        "            return 0.5  # Значение по умолчанию при ошибке\n",
        "\n",
        "    def _create_topic_text(self, topic_data: pd.Series) -> str:\n",
        "        \"\"\"Создание единого текстового представления темы проекта\"\"\"\n",
        "        text_parts = []\n",
        "        relevant_columns = ['title', 'description', 'required_specializations', 'tasks', 'goals']\n",
        "\n",
        "        for col in relevant_columns:\n",
        "            if col in topic_data and pd.notna(topic_data[col]):\n",
        "                text_parts.append(str(topic_data[col]))\n",
        "\n",
        "        return \" \".join(text_parts)\n",
        "\n",
        "    def _extract_keywords(self, topic_data: pd.Series) -> List[str]:\n",
        "        \"\"\"Извлечение ключевых слов из описания темы\"\"\"\n",
        "        text = self._create_topic_text(topic_data)\n",
        "        # Извлекаем слова длиной от 3 символов, включая русские и английские\n",
        "        words = re.findall(r'\\b[a-zA-Zа-яА-ЯёЁ]{3,}\\b', text.lower())\n",
        "        return list(set(words))  # Убираем дубликаты\n",
        "\n",
        "    def calculate_semantic_similarity(self, student_texts: List[str], topic_text: str) -> np.ndarray:\n",
        "        \"\"\"Вычисление семантической схожести между студентами и темой проекта\"\"\"\n",
        "        # Используем кэш для избежания повторных вычислений эмбеддингов\n",
        "        cache_key = hash(tuple(student_texts))\n",
        "\n",
        "        if cache_key not in self._student_embeddings_cache:\n",
        "            # Пакетное вычисление эмбеддингов для всех студентов\n",
        "            student_embeddings = self.model.encode(student_texts, show_progress_bar=False)\n",
        "            self._student_embeddings_cache[cache_key] = student_embeddings\n",
        "        else:\n",
        "            student_embeddings = self._student_embeddings_cache[cache_key]\n",
        "\n",
        "        # Вычисляем эмбеддинг для темы (один раз)\n",
        "        topic_embedding = self.model.encode([topic_text])\n",
        "\n",
        "        # Вычисляем косинусное сходство\n",
        "        similarities = cosine_similarity(student_embeddings, topic_embedding)\n",
        "        return similarities.flatten()\n",
        "\n",
        "    @lru_cache(maxsize=128)\n",
        "    def calculate_specialization_match(self, student_spec: str, required_spec: str) -> float:\n",
        "        \"\"\"Оценка соответствия специализаций с учетом смежных областей\"\"\"\n",
        "        if student_spec == required_spec:\n",
        "            return 1.0\n",
        "\n",
        "        # Граф связанных специализаций\n",
        "        related_specs = {\n",
        "            'Machine Learning': ['Data Science', 'NLP', 'Computer Vision', 'Data Analytics'],\n",
        "            'Data Science': ['Machine Learning', 'Data Engineering', 'NLP', 'Product Analytics'],\n",
        "            'NLP': ['Machine Learning', 'Data Science'],\n",
        "            'Computer Vision': ['Machine Learning', 'Data Science'],\n",
        "            'Backend': ['DevOps', 'Data Engineering'],\n",
        "            'Frontend': ['UI/UX', 'Android'],\n",
        "            'Android': ['Frontend', 'UI/UX'],\n",
        "            'DevOps': ['Backend', 'Data Engineering'],\n",
        "            'Data Engineering': ['Data Science', 'Backend', 'DevOps'],\n",
        "            'UI/UX': ['Frontend', 'Android']\n",
        "        }\n",
        "\n",
        "        # Проверяем двунаправленные связи\n",
        "        if student_spec in related_specs and required_spec in related_specs[student_spec]:\n",
        "            return 0.7\n",
        "\n",
        "        if required_spec in related_specs and student_spec in related_specs[required_spec]:\n",
        "            return 0.7\n",
        "\n",
        "        return 0.0\n",
        "\n",
        "    def calculate_skill_match(self, student_skills: List[str], topic_keywords: List[str]) -> float:\n",
        "        \"\"\"Оценка соответствия навыков студента требованиям темы\"\"\"\n",
        "        if not student_skills:\n",
        "            return 0.0\n",
        "\n",
        "        # Преобразуем в множества для быстрого поиска\n",
        "        student_skills_set = set(student_skills)\n",
        "        topic_keywords_set = set(topic_keywords)\n",
        "\n",
        "        # Ищем пересечения навыков и ключевых слов\n",
        "        matches = len([skill for skill in student_skills_set\n",
        "                      if any(skill in kw or kw in skill for kw in topic_keywords_set)])\n",
        "\n",
        "        return matches / len(student_skills_set)\n",
        "\n",
        "    def calculate_comprehensive_score(self, semantic_similarity: float, spec_match: float,\n",
        "                                     skill_match: float, hours_score: float,\n",
        "                                     weights: Optional[Dict] = None) -> float:\n",
        "        \"\"\"Комплексная оценка соответствия студента теме проекта\"\"\"\n",
        "        if weights is None:\n",
        "            # Веса можно настраивать в зависимости от приоритетов\n",
        "            weights = {'semantic': 0.4, 'specialization': 0.3, 'skills': 0.2, 'hours': 0.1}\n",
        "\n",
        "        # Взвешенная сумма всех метрик\n",
        "        score = (semantic_similarity * weights['semantic'] +\n",
        "                spec_match * weights['specialization'] +\n",
        "                skill_match * weights['skills'] +\n",
        "                hours_score * weights['hours'])\n",
        "\n",
        "        return min(1.0, score)  # Ограничиваем максимальным значением 1.0\n",
        "\n",
        "    def preprocess_student_data(self, student_data: pd.DataFrame) -> Tuple[pd.DataFrame, List[str]]:\n",
        "        \"\"\"Предобработка данных студентов: нормализация и извлечение признаков\"\"\"\n",
        "        df = student_data.copy()\n",
        "\n",
        "        # Определяем имена столбцов автоматически\n",
        "        col_names = df.columns.tolist()\n",
        "\n",
        "        # Поиск ID студента\n",
        "        id_col = next((col for col in col_names if 'id' in col.lower()), col_names[0])\n",
        "\n",
        "        # Удаляем строки без ID\n",
        "        df = df.dropna(subset=[id_col])\n",
        "\n",
        "        # Конвертируем ID в числовой формат\n",
        "        df['student_id_int'] = df[id_col].apply(self._safe_convert_to_int)\n",
        "\n",
        "        # Поиск нужных столбцов по ключевым словам\n",
        "        spec_col = next((col for col in col_names if 'специал' in col.lower()),\n",
        "                       next((col for col in col_names if 'special' in col.lower()), col_names[1]))\n",
        "\n",
        "        exp_col = next((col for col in col_names if 'опыт' in col.lower() or 'experience' in col.lower()),\n",
        "                      col_names[2])\n",
        "\n",
        "        int_col = next((col for col in col_names if 'интерес' in col.lower() or 'interest' in col.lower()),\n",
        "                      col_names[3])\n",
        "\n",
        "        hours_col = next((col for col in col_names if any(word in col.lower() for word in ['врем', 'час', 'time'])),\n",
        "                        col_names[4])\n",
        "\n",
        "        # Применяем нормализацию и извлечение признаков\n",
        "        df['specialization_clean'] = df[spec_col].apply(self._normalize_specialization)\n",
        "        df['skills'] = df[exp_col].apply(self._extract_skills)\n",
        "        df['hours_normalized'] = df[hours_col].apply(self._normalize_hours)\n",
        "\n",
        "        # Создаем список текстовых представлений для семантического анализа\n",
        "        student_texts = [\n",
        "            f\"{row[spec_col]} {row[exp_col]} {row[int_col]}\"\n",
        "            for _, row in df.iterrows()\n",
        "        ]\n",
        "\n",
        "        return df, student_texts, [id_col, spec_col, exp_col, int_col, hours_col]\n",
        "\n",
        "    def _safe_convert_to_int(self, value: Any) -> int:\n",
        "        \"\"\"Безопасное преобразование значения в целое число\"\"\"\n",
        "        try:\n",
        "            if pd.isna(value):\n",
        "                return 0\n",
        "\n",
        "            value_str = str(value).strip()\n",
        "            numbers = re.findall(r'\\d+', value_str)\n",
        "\n",
        "            if numbers:\n",
        "                return int(numbers[0])\n",
        "\n",
        "            # Пробуем преобразовать как число с плавающей точкой\n",
        "            return int(float(value))\n",
        "        except (ValueError, TypeError):\n",
        "            return 0\n",
        "\n",
        "    def preprocess_topic_data(self, topic_data: pd.Series) -> Dict[str, Any]:\n",
        "        \"\"\"Предобработка данных темы проекта\"\"\"\n",
        "        processed_topic = topic_data.to_dict()\n",
        "\n",
        "        # Обрабатываем требуемые специализации\n",
        "        if 'required_specializations' in processed_topic and pd.notna(processed_topic['required_specializations']):\n",
        "            required_specs = [s.strip() for s in str(processed_topic['required_specializations']).split(',')]\n",
        "            processed_topic['required_specializations_list'] = required_specs\n",
        "        else:\n",
        "            processed_topic['required_specializations_list'] = []\n",
        "\n",
        "        # Создаем полное текстовое описание и извлекаем ключевые слова\n",
        "        processed_topic['full_description'] = self._create_topic_text(topic_data)\n",
        "        processed_topic['keywords'] = self._extract_keywords(topic_data)\n",
        "\n",
        "        return processed_topic\n",
        "\n",
        "    def find_best_students_for_specialization(self, students_data: pd.DataFrame,\n",
        "                                             topic_data: pd.Series,\n",
        "                                             required_spec: str,\n",
        "                                             top_k: int = 5) -> List[Dict]:\n",
        "        \"\"\"Поиск лучших студентов для конкретной специализации темы\"\"\"\n",
        "        # Предобработка данных\n",
        "        students_df, student_texts, cols = self.preprocess_student_data(students_data)\n",
        "        id_col, spec_col, exp_col, int_col, hours_col = cols\n",
        "\n",
        "        processed_topic = self.preprocess_topic_data(topic_data)\n",
        "\n",
        "        # Вычисление семантической схожести (оптимизированная версия)\n",
        "        semantic_scores = self.calculate_semantic_similarity(\n",
        "            student_texts,\n",
        "            processed_topic['full_description']\n",
        "        )\n",
        "\n",
        "        results = []\n",
        "\n",
        "        # Оценка каждого студента\n",
        "        for i, (_, student) in enumerate(students_df.iterrows()):\n",
        "            # Соответствие специализации\n",
        "            spec_match = self.calculate_specialization_match(\n",
        "                student['specialization_clean'],\n",
        "                required_spec\n",
        "            )\n",
        "\n",
        "            # Соответствие навыков\n",
        "            skill_match = self.calculate_skill_match(\n",
        "                student['skills'],\n",
        "                processed_topic['keywords']\n",
        "            )\n",
        "\n",
        "            # Комплексная оценка\n",
        "            comprehensive_score = self.calculate_comprehensive_score(\n",
        "                semantic_similarity=semantic_scores[i],\n",
        "                spec_match=spec_match,\n",
        "                skill_match=skill_match,\n",
        "                hours_score=student['hours_normalized']\n",
        "            )\n",
        "\n",
        "            # Ключевое исправление: преобразуем student_id в целое число\n",
        "            raw_student_id = student[id_col]\n",
        "            try:\n",
        "                # Преобразуем в целое число, если это возможно\n",
        "                if isinstance(raw_student_id, (int, np.integer)):\n",
        "                    student_id_int = int(raw_student_id)\n",
        "                elif isinstance(raw_student_id, (float, np.floating)):\n",
        "                    student_id_int = int(raw_student_id)\n",
        "                else:\n",
        "                    # Для строк пытаемся извлечь число\n",
        "                    numbers = re.findall(r'\\d+', str(raw_student_id))\n",
        "                    student_id_int = int(numbers[0]) if numbers else int(float(raw_student_id))\n",
        "            except (ValueError, TypeError, IndexError):\n",
        "                # Если не удается преобразовать, используем внутренний ID\n",
        "                student_id_int = int(student['student_id_int'])\n",
        "\n",
        "            # Сохраняем результат\n",
        "            result = {\n",
        "                'student_id': student_id_int,\n",
        "                'student_specialization': student[spec_col],\n",
        "                'normalized_specialization': student['specialization_clean'],\n",
        "                'required_specialization': required_spec,\n",
        "                'comprehensive_score': float(comprehensive_score),\n",
        "                'semantic_similarity': float(semantic_scores[i]),\n",
        "                'specialization_match': float(spec_match),\n",
        "                'skills_match': float(skill_match),\n",
        "                'available_hours': float(student['hours_normalized'])\n",
        "            }\n",
        "            results.append(result)\n",
        "\n",
        "        # Сортировка по комплексной оценке\n",
        "        results.sort(key=lambda x: x['comprehensive_score'], reverse=True)\n",
        "        return results[:top_k]\n",
        "\n",
        "    def create_topic_specialization_map(self, students_data: pd.DataFrame,\n",
        "                                       topics_data: pd.DataFrame,\n",
        "                                       top_k_per_spec: int = 5) -> Dict:\n",
        "        \"\"\"Создание карты соответствия тем проектов и кандидатов по специализациям\"\"\"\n",
        "        topic_specialization_map = {}\n",
        "\n",
        "        # Обрабатываем каждую тему\n",
        "        for topic_idx, topic in topics_data.iterrows():\n",
        "            # Извлекаем или генерируем ID темы\n",
        "            topic_id = topic.get('topic_id', topic.get('id', f'topic_{topic_idx + 1}'))\n",
        "            topic_id_int = self._safe_convert_to_int(topic_id)\n",
        "            topic_title = topic.get('title', 'Без названия')\n",
        "\n",
        "            # Определяем требуемые специализации\n",
        "            if 'required_specializations' in topic and pd.notna(topic['required_specializations']):\n",
        "                required_specs = [s.strip() for s in str(topic['required_specializations']).split(',')]\n",
        "            else:\n",
        "                required_specs = []\n",
        "\n",
        "            candidates_by_specialization = {}\n",
        "\n",
        "            # Ищем лучших кандидатов для каждой требуемой специализации\n",
        "            for required_spec in required_specs:\n",
        "                best_students = self.find_best_students_for_specialization(\n",
        "                    students_data, topic, required_spec, top_k=top_k_per_spec\n",
        "                )\n",
        "\n",
        "                # Сохраняем только ID студентов (теперь целые числа)\n",
        "                student_ids = [student['student_id'] for student in best_students]\n",
        "                candidates_by_specialization[required_spec] = student_ids\n",
        "\n",
        "            # Сохраняем результаты для этой темы\n",
        "            topic_specialization_map[topic_id_int] = {\n",
        "                'title': topic_title,\n",
        "                'candidates_by_specialization': candidates_by_specialization\n",
        "            }\n",
        "\n",
        "        return topic_specialization_map\n",
        "\n",
        "    def get_formatted_output(self, topic_specialization_map: Dict) -> Dict:\n",
        "        \"\"\"Форматирование результата для сохранения\"\"\"\n",
        "        formatted_output = {}\n",
        "\n",
        "        for topic_id, topic_info in topic_specialization_map.items():\n",
        "            project_key = f\"Проект{topic_id}\"\n",
        "            formatted_output[project_key] = topic_info['candidates_by_specialization']\n",
        "\n",
        "        return formatted_output\n",
        "\n",
        "    def save_results_to_json(self, topic_specialization_map: Dict,\n",
        "                            filename: str = \"topic_candidate_map.json\") -> Dict:\n",
        "        \"\"\"Сохранение результатов в JSON файл\"\"\"\n",
        "        formatted_data = self.get_formatted_output(topic_specialization_map)\n",
        "\n",
        "        with open(filename, 'w', encoding='utf-8') as f:\n",
        "            json.dump(formatted_data, f, ensure_ascii=False, indent=2)\n",
        "\n",
        "        return formatted_data\n",
        "\n",
        "matcher = CSVStudentTopicMatcher()\n",
        "\n",
        "topic_specialization_map = matcher.create_topic_specialization_map(\n",
        "    students_df,\n",
        "    topics_df,\n",
        "    top_k_per_spec=5\n",
        ")\n",
        "\n",
        "final_data = matcher.save_results_to_json(topic_specialization_map)\n",
        "files.download(\"topic_candidate_map.json\")"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
